# Base image with Java
FROM openjdk:8-jdk

# Install additional dependencies
RUN apt-get update && \
    apt-get install -y curl wget && \
    apt-get clean

# Set environment variables
ENV JAVA_HOME /usr/lib/jvm/java-8-openjdk-amd64
ENV SPARK_HOME /opt/spark
ENV HADOOP_HOME /opt/hadoop
ENV MAHOUT_HOME /opt/mahout

# Install Apache Spark
RUN wget -qO- https://downloads.apache.org/spark/spark-3.2.0/spark-3.2.0-bin-hadoop3.2.tgz | tar -xz -C /opt && \
    mv /opt/spark-3.2.0-bin-hadoop3.2 $SPARK_HOME

# Install Apache Hadoop
RUN wget -qO- https://downloads.apache.org/hadoop/common/hadoop-3.3.1/hadoop-3.3.1.tar.gz | tar -xz -C /opt && \
    mv /opt/hadoop-3.3.1 $HADOOP_HOME

# Install Apache Mahout
RUN wget -qO- https://downloads.apache.org/mahout/0.13.0/apache-mahout-distribution-0.13.0.tar.gz | tar -xz -C /opt && \
    mv /opt/apache-mahout-distribution-0.13.0 $MAHOUT_HOME

# Add Spark and Hadoop to PATH
ENV PATH $SPARK_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PATH

# Set Spark and Hadoop configuration
COPY spark-env.sh $SPARK_HOME/conf/spark-env.sh
COPY core-site.xml $HADOOP_HOME/etc/hadoop/core-site.xml
COPY hdfs-site.xml $HADOOP_HOME/etc/hadoop/hdfs-site.xml
COPY mapred-site.xml $HADOOP_HOME/etc/hadoop/mapred-site.xml
COPY yarn-site.xml $HADOOP_HOME/etc/hadoop/yarn-site.xml

# Set working directory
WORKDIR /app

# Copy your application files to the container
COPY . /app

# Build and run your application
# Example:
# RUN javac YourApp.java
# CMD java YourApp

# Cleanup unnecessary files
RUN rm -rf $SPARK_HOME/examples $SPARK_HOME/licenses $HADOOP_HOME/share/doc $MAHOUT_HOME/examples $MAHOUT_HOME/docs

# Expose Spark UI and Hadoop UI ports
EXPOSE 4040 8088

# Cleanup apt-get cache
RUN apt-get clean

# Set the entrypoint to start the Spark shell by default
ENTRYPOINT ["spark-shell"]
